# 6. Documentar o incidente e as melhorias preventivas

## üéØ Objetivo
Registrar de forma clara e estruturada as informa√ß√µes relacionadas ao incidente, incluindo contexto, linha do tempo, causa raiz, a√ß√µes de mitiga√ß√£o e melhorias preventivas.  
Essa documenta√ß√£o serve como base para aprendizado cont√≠nuo, revis√£o de processos e prepara√ß√£o para futuras ocorr√™ncias similares.

---

# üìÑ Postmortem ‚Äî Incidente de Lat√™ncia no `payment-api`

## üóì Data e Hora
- **In√≠cio:** 31/09/2025 ‚Äî 09:15 BRT  
- **Resolu√ß√£o:** 31/09/2025 ‚Äî 10:02 BRT  
- **Dura√ß√£o:** 47 minutos  
- **Ambiente:** Produ√ß√£o  
- **Servi√ßo:** `payment-api`  
- **Detec√ß√£o:** Datadog  
- **Alerta cr√≠tico:**  
  `ALERTA: Lat√™ncia m√©dia da API acima de 2 segundos por 10 minutos`  
- **Impacto:**  
  - Lat√™ncia m√©dia > 2 segundos por 10 minutos  
  - Timeouts em requisi√ß√µes ao RDS  
  - Indisponibilidade parcial do processamento de pagamentos  
  - Impacto confirmado no faturamento pela equipe de neg√≥cios

---

## üîç Linha do Tempo

| Hor√°rio | Evento |
|--------:|--------|
| 09:15   | Lat√™ncia da API come√ßa a subir acima de 2s |
| 09:16   | Alerta Datadog disparado |
| 09:18   | SRE confirma impacto e aciona Dev + DBA |
| 09:20   | Verifica√ß√£o de pods e m√©tricas no Kubernetes ‚Äî todos Running |
| 09:25   | Logs mostram timeouts ao acessar RDS |
| 09:28   | M√©tricas do RDS indicam conex√µes no limite e aumento de CPU |
| 09:30   | Escalado n√∫mero de r√©plicas do `payment-api` de 6 para 10 |
| 09:35   | DBA aumenta temporariamente capacidade do RDS |
| 09:45   | Lat√™ncia come√ßa a cair gradualmente |
| 10:02   | M√©tricas normalizadas, incidente encerrado |

---

## üßæ Causa Raiz
A aplica√ß√£o de alta criticidade, estava sendo executada em um cluster padr√£o com recursos limitados e configura√ß√£o de autoscaling inadequada. Tamb√©m foi analiso que a inst√¢ncia do RDS apresentava capacidade subdimensionada para lidar com picos de carga. Embora na maioria do tempo o sistema operava em boas condi√ß√µes, mas com aumento repentino no volume de requisi√ß√µes houve um gargalho.
Em resumo a lentid√£o na resposta do autoscaling do EKS e a falta de recurso do RDS, resultou na satura√ß√£o de CPU e mem√≥ria nos nodes do cluster. Isso levou ao throttling dos pods e ao aumento significativo da lat√™ncia no processamento das requisi√ß√µes, impactando diretamente a disponibilidade do servi√ßo.

---

## üöë A√ß√µes Imediatas (Mitiga√ß√£o)
- Escalonamento horizontal dos nodes de forma manual para aliviar a carga e estabilizar o servi√ßo.
- Aumento de da capacidade do RDS
---

## üîß Melhorias Preventivas
- Migrar a aplica√ß√£o para outro cluster ou nodegroup com capacidade adequada √† sua criticidade.
- Explorar recurso RDS com time de Banco de dados para atender melhor o ambiente nesses cen√°rios.
- Criar dashboards com vis√£o √∫nica das principais m√©tricas de infraestrutura do EKS e RDS, servi√ßo, neg√≥cio e logs.
  - Disponibilizar esses dashboards ao NOC para acompanhamento em tempo real.
  - Utilizar como suporte em futuras an√°lises de incidentes.
