# 6. Documentar o incidente e as melhorias preventivas

## ğŸ¯ Objetivo
Registrar de forma clara e estruturada as informaÃ§Ãµes relacionadas ao incidente, incluindo contexto, linha do tempo, causa raiz, aÃ§Ãµes de mitigaÃ§Ã£o e melhorias preventivas.  
Essa documentaÃ§Ã£o serve como base para aprendizado contÃ­nuo, revisÃ£o de processos e preparaÃ§Ã£o para futuras ocorrÃªncias similares.

---

# ğŸ“„ Postmortem â€” Incidente de LatÃªncia no `payment-api`

## ğŸ—“ Data e Hora
- **InÃ­cio:** 31/09/2025 â€” 09:15 BRT  
- **ResoluÃ§Ã£o:** 31/09/2025 â€” 10:02 BRT  
- **DuraÃ§Ã£o:** 47 minutos  
- **Ambiente:** ProduÃ§Ã£o  
- **ServiÃ§o:** `payment-api`  
- **DetecÃ§Ã£o:** Datadog  
- **Alerta crÃ­tico:**  
  `ALERTA: LatÃªncia mÃ©dia da API acima de 2 segundos por 10 minutos`  
- **Impacto:**  
  - LatÃªncia mÃ©dia > 2 segundos por 10 minutos  
  - Timeouts em requisiÃ§Ãµes ao RDS  
  - Indisponibilidade parcial do processamento de pagamentos  
  - Impacto confirmado no faturamento pela equipe de negÃ³cios

---

## ğŸ” Linha do Tempo

| HorÃ¡rio | Evento |
|--------:|--------|
| 09:15   | LatÃªncia da API comeÃ§a a subir acima de 2s |
| 09:16   | Alerta Datadog disparado |
| 09:18   | SRE confirma impacto e aciona Dev + DBA |
| 09:20   | VerificaÃ§Ã£o de pods e mÃ©tricas no Kubernetes â€” todos Running |
| 09:25   | Logs mostram timeouts ao acessar RDS |
| 09:28   | MÃ©tricas do RDS indicam conexÃµes no limite e aumento de CPU |
| 09:30   | Escalado nÃºmero de rÃ©plicas do `payment-api` de 6 para 10 |
| 09:35   | DBA aumenta temporariamente capacidade do RDS |
| 09:45   | LatÃªncia comeÃ§a a cair gradualmente |
| 10:02   | MÃ©tricas normalizadas, incidente encerrado |

---

## ğŸ§¾ Causa Raiz
A aplicaÃ§Ã£o de alta criticidade estava sendo executada em um cluster padrÃ£o com recursos limitados e autoscaling inadequado.  
O volume inesperado de requisiÃ§Ãµes causou saturaÃ§Ã£o de CPU e memÃ³ria nos nodes do EKS, levando a throttling e aumento de latÃªncia no processamento das requisiÃ§Ãµes.

---

## ğŸš‘ AÃ§Ãµes Imediatas (MitigaÃ§Ã£o)
- Escalonamento horizontal dos nodes de forma manual para aliviar a carga e estabilizar o serviÃ§o.

---

## ğŸ”§ Melhorias Preventivas
- Migrar a aplicaÃ§Ã£o para outro cluster ou nodegroup com capacidade adequada Ã  sua criticidade.
- Criar dashboards com mÃ©tricas de infraestrutura, serviÃ§o, negÃ³cio e logs.
  - Disponibilizar esses dashboards ao NOC para acompanhamento em tempo real.
  - Utilizar como suporte em futuras anÃ¡lises de incidentes.
